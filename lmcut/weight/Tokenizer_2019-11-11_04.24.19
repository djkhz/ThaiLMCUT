-----------Tokenizer---------
>>> command with full parameters
python Tokenizer.py --adam_lr_decay 0.0 --add_note 'load from LM_2019-10-19_17.05.50  --best_full  --lr 0.0001 --epoch 20' --batchSize 64 --char_dropout_prob 0.01 --char_embedding_size 200 --clip_grad 0.5 --dataset_size 'best_full' --epoch 20 --hidden_dim 514 --layer_num 2 --learning_rate 0.0001 --len_lines_per_chunk 1000 --load_from 'LM_2019-10-19_17.05.50' --lr_decay 0.01 --lstm_num_direction 2 --optim 'adam' --over_write 0 --printPrediction 0 --save_to 'Tokenizer_2019-11-11_04.24.19' --sequence_length 150 --sgd_momentum 0.02 --with_dot 0

>>> command with short parameters
python Tokenizer.py --batchSize 64 --char_dropout_prob 0.01 --char_embedding_size 200 --dataset_size best_full --epoch 20 --hidden_dim 514 --layer_num 2 --learning_rate 0.0001 --len_lines_per_chunk 1000 --lstm_num_direction 2 --optim adam --sequence_length 150 

file name =  Tokenizer_2019-11-11_04.24.19

>>>  load from LM_2019-10-19_17.05.50 , best_full , lr 0.0001, epoch 20
bi_lstm  True
train classifier for 20 epoch
trainLosses  [0.08586042789318184, 0.037716667292326096, 0.029989893416791123, 0.025696575526369385, 0.022700542644574537, 0.020487995896635965, 0.018740565409436638, 0.017333791447148687, 0.016178300099373603, 0.015095994652751464, 0.0142455046540034, 0.013479661970459257, 0.012805565124917038]
devLosses  [0.04142784149344288, 0.031870215093318506, 0.02772125051315151, 0.025471382348135632, 0.024158611434942752, 0.023189853316182223, 0.022445717112935006, 0.022101663419077943, 0.021806057616697613, 0.02151694857696099, 0.021436091521708528, 0.021275859849703548, 0.021299096968821922]

count train sample  103424
count dev sample  12864

adam_lr_decay=0.0
 add_note='load from LM_2019-10-19_17.05.50 
 best_full 
 lr 0.0001
 epoch 20'
 batchSize=64
 char_dropout_prob=0.01
 char_embedding_size=200
 clip_grad=0.5
 dataset_size='best_full'
 epoch=20
 hidden_dim=514
 layer_num=2
 learning_rate=0.0001
 len_lines_per_chunk=1000
 load_from='LM_2019-10-19_17.05.50'
 lr_decay=0.01
 lstm_num_direction=2
 optim='adam'
 over_write=0
 printPrediction=0
 save_to='Tokenizer_2019-11-11_04.24.19'
 sequence_length=150
 sgd_momentum=0.02
 with_dot=0

train set:  ./data/BEST_dataset/BEST_dot_full_train_0.8.txt
dev set:  ./data/BEST_dataset/BEST_dot_full_dev_0.1.txt
test set:  ./data/BEST_dataset/BEST_dot_full_test_0.1.txt

count train sample  103424
count dev sample  12864
config for later download : 
--batchSize 64 --char_dropout_prob 0.01 --char_embedding_size 200 --hidden_dim 514 --layer_num 2 --learning_rate 0.0001 --len_lines_per_chunk 1000 --lstm_num_direction 2 --optim adam --load_from Tokenizer_2019-11-11_04.24.19 --sequence_length 150 

----------resume training/early stopping ---------
>>> command with full parameters
python Tokenizer.py --adam_lr_decay 0.0 --add_note 'load from LM_2019-10-19_17.05.50  --best_full  --lr 0.0001 --epoch 20' --batchSize 64 --char_dropout_prob 0.01 --char_embedding_size 200 --clip_grad 0.5 --dataset_size 'best_full' --epoch 20 --hidden_dim 514 --layer_num 2 --learning_rate 0.0001 --len_lines_per_chunk 1000 --load_from 'LM_2019-10-19_17.05.50' --lr_decay 0.01 --lstm_num_direction 2 --optim 'adam' --over_write 0 --printPrediction 0 --save_to 'Tokenizer_2019-11-11_04.24.19' --sequence_length 150 --sgd_momentum 0.02 --with_dot 0

>>> command with short parameters
python Tokenizer.py --batchSize 64 --char_dropout_prob 0.01 --char_embedding_size 200 --dataset_size best_full --epoch 20 --hidden_dim 514 --layer_num 2 --learning_rate 0.0001 --len_lines_per_chunk 1000 --lstm_num_direction 2 --optim adam --sequence_length 150 

file name =  Tokenizer_2019-11-11_04.24.19

>>>  load from LM_2019-10-19_17.05.50 , best_full , lr 0.0001, epoch 20
bi_lstm  True
train classifier for 20 epoch
trainLosses  [0.08586042789318184, 0.037716667292326096, 0.029989893416791123, 0.025696575526369385, 0.022700542644574537, 0.020487995896635965, 0.018740565409436638, 0.017333791447148687, 0.016178300099373603, 0.015095994652751464, 0.0142455046540034, 0.013479661970459257, 0.012805565124917038]
devLosses  [0.04142784149344288, 0.031870215093318506, 0.02772125051315151, 0.025471382348135632, 0.024158611434942752, 0.023189853316182223, 0.022445717112935006, 0.022101663419077943, 0.021806057616697613, 0.02151694857696099, 0.021436091521708528, 0.021275859849703548, 0.021299096968821922]

count train sample  103424
count dev sample  12864

adam_lr_decay=0.0
 add_note='load from LM_2019-10-19_17.05.50 
 best_full 
 lr 0.0001
 epoch 20'
 batchSize=64
 char_dropout_prob=0.01
 char_embedding_size=200
 clip_grad=0.5
 dataset_size='best_full'
 epoch=20
 hidden_dim=514
 layer_num=2
 learning_rate=0.0001
 len_lines_per_chunk=1000
 load_from='LM_2019-10-19_17.05.50'
 lr_decay=0.01
 lstm_num_direction=2
 optim='adam'
 over_write=0
 printPrediction=0
 save_to='Tokenizer_2019-11-11_04.24.19'
 sequence_length=150
 sgd_momentum=0.02
 with_dot=0

train set:  ./data/BEST_dataset/BEST_dot_full_train_0.8.txt
dev set:  ./data/BEST_dataset/BEST_dot_full_dev_0.1.txt
test set:  ./data/BEST_dataset/BEST_dot_full_test_0.1.txt

count train sample  103424
count dev sample  12864
config for later download : 
--batchSize 64 --char_dropout_prob 0.01 --char_embedding_size 200 --hidden_dim 514 --layer_num 2 --learning_rate 0.0001 --len_lines_per_chunk 1000 --lstm_num_direction 2 --optim adam --load_from Tokenizer_2019-11-11_04.24.19 --sequence_length 150 

falseNegatives  5996
falsePositives  7043
correct  461308
Boundary measures: 
Precision 0.984962133101029 Recall 0.9871689521168233 F1 0.9860643078912633

Word measures Precision 0.973248695956665 Recall 0.9754292708814819 F1 0.9743377633850084
word_correct : 455822
count_predict_word:  468351
count_real_word:  467304

สดง ถึง ความ หนักแน่น อดทน ของ ภาค รัฐ ผู้ เลือก ใช้ วิถีทาง สันติ แม้ ต้อง เสียสละ ชีวิต ของ เจ้าหน้าที่ จน ทำ ให้ ภาค รัฐ ประสบ ชัยชนะ ยิ่งใหญ่ ทาง การ เมือง ตุลาคม จาก สารานุกรม มห
สดง ถึง ความ หนักแน่น อด ทน ของ ภาค รัฐ ผู้ เลือก ใช้ วิถีทาง สันติ แม้ ต้อง เสียสละ ชีวิต ของ เจ้าหน้าที่ จน ทำ ให้ ภาค รัฐ ประสบ ชัยชนะ ยิ่งใหญ่ ทาง การ เมือง ตุลาคม จาก สารานุกรม มห
7:02:52.303254
